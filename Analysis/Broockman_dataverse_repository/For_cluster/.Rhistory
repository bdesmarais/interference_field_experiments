q[12] <- q[11] + p[12]
q
30/500
70/500
120/500
280/500
24*4
55*4
5/0.06
12/0.14
24/0.24
55/0.56
(20-96)^2
(48-96)^2
(96-96)^2
(220-96)^2
((5/0.06)-96)^2
((12/0.14)-96)^2
((24/0.24)-96)^2
((55/0.56)-96)^2
1222+1283+1467+1354+1540+1523
8389/6
(2*(1800-1400)^2)+(2*(1600-1400)^2)
((2*(1800-1400)^2)+(2*(1600-1400)^2))/6
((25000-4)/(25000*4))*(((150-147.5)^2+(100-147.5)^2+(200-147.5)^2+(140-147.5)^2)/3)
72500/25000
(147.5*2.9)/3.25
m <- 131.6154
((25000-4)/(25000*4))*(((150-m)^2+(100-m)^2+(200-m)^2+(140-m)^2)/3)
60*2.5/22.5
60*1.5/17.5
60*1.5/12.5
60*0.5/7.5
0.5^2
1.5^2
2.25*2
4.5*4
qt(0.025, 9.5)
qt(0.025, 9.7)
x1 <- sample(0:1, size = 500, replace = TRUE, prob = c(0.7,0.3))
x2 <- sample(0:1, size = 500, replace = TRUE, prob = c(0.245,755))
y <- rnorm(n = 500, mean = 4.5, sd = 3.25)
summary(lm(y~x1+x2))
summary(lm(y~x1*x2))
x1 <- sample(0:1, size = 500, replace = TRUE, prob = c(0.7,0.3))
x2 <- sample(3:4, size = 500, replace = TRUE, prob = c(0.245,0.755))
y <- rnorm(n = 500, mean = 4.5, sd = 3.25)
summary(lm(y~x1*x2))
x1 <- sample(0:4, size = 5000, replace = TRUE, prob = c(0.5,0.2,0.15,0.15))
x2 <- sample(1:5, size = 5000, replace = TRUE, prob = c(0.2,0.045,0.655,0.1))
x1 <- sample(0:4, size = 5000, replace = TRUE, prob = c(0.4,0.1,0.2,0.15,0.15))
x2 <- sample(1:5, size = 5000, replace = TRUE, prob = c(0.15,0.05,0.045,0.655,0.1))
y <- rnorm(n = 5000, mean = 4.5, sd = 3.25)
summary(lm(y~x1*x2))
summary(lm(y~as.factor(x1)*as.factor(x2)))
x1 <- sample(0:4, size = 5000, replace = TRUE, prob = c(0.4,0.1,0.2,0.15,0.15))
x2 <- sample(1:5, size = 5000, replace = TRUE, prob = c(0.15,0.05,0.045,0.655,0.1))
x3 <- sample(0:4, size = 5000, replace = TRUE, prob = c(0.25,0.25,0.25,0.25))
x1 <- sample(0:4, size = 5000, replace = TRUE, prob = c(0.4,0.1,0.2,0.15,0.15))
x2 <- sample(1:5, size = 5000, replace = TRUE, prob = c(0.15,0.05,0.045,0.655,0.1))
x3 <- sample(0:4, size = 5000, replace = TRUE, prob = c(0.2,0.2,0.2,0.2,0.2))
x4 <- sample(1:5, size = 5000, replace = TRUE, prob = c(0.15,0.05,0.045,0.655,0.1))
y <- rnorm(n = 5000, mean = 4.5, sd = 3.25)
summary(lm(y~as.factor(x1)*as.factor(x2)*as.factor(x3)*as.factor(x4)))
summary(lm(y~as.factor(x1)*as.factor(x2)*as.factor(x3)*as.factor(x4)))$coef
x1 <- sample(c(A,B,C,D,E), size = 5000, replace = TRUE, prob = c(0.4,0.1,0.2,0.15,0.15))
x2 <- sample(c(A,B,C,D,E), size = 5000, replace = TRUE, prob = c(0.15,0.05,0.045,0.655,0.1))
x3 <- sample(c(A,B,C,D,E), size = 5000, replace = TRUE, prob = c(0.2,0.2,0.2,0.2,0.2))
x1 <- sample(0:4, size = 5000, replace = TRUE, prob = c(0.4,0.1,0.2,0.15,0.15))
x2 <- sample(0:4, size = 5000, replace = TRUE, prob = c(0.15,0.05,0.045,0.655,0.1))
x3 <- sample(0:4, size = 5000, replace = TRUE, prob = c(0.2,0.2,0.2,0.2,0.2))
x4 <- sample(0:4, size = 5000, replace = TRUE, prob = c(0.15,0.05,0.045,0.655,0.1))
y <- rnorm(n = 5000, mean = 4.5, sd = 3.25)
summary(lm(y~as.factor(x1)*as.factor(x2)*as.factor(x3)*as.factor(x4)))
(50*(10+20+30))/(150+300)
(2800+700+600)/(450^2)
((280000)+(300*5*600))/(450^2)
sum(n)
L <- 3
N <- c(50,80,60)
n <- c(14,20,16)
sum(n)
sum(N)
samp1 <- c(80,68,72,85,90,69,65,92,88,89,91,84,79,88)
samp2 <- c(67,48,53,65,49,72,50,68,70,63,83,75,73,78,69,81,63,52,61,45)
samp3 <- c(42,36,55,44,53,61,42,35,32,36,29,19,19,31,30,35)
summary(samp1,samp2,samp3)
sapply(c(samp1,samp2,samp3), summary)
summary(samp1)
summary(samp2)
summary(samp3)
?var
var(samp1)
mean(samp1)
sd(samp1)
sd(samp2)
sd(samp3)
rbind(samp1,samp2,samp3)
c(samp1,samp2,samp3)
mean(c(samp1,samp2,samp3))
sd(c(samp1,samp2,samp3))
6004880*10
6004880*10)
6004880*100
6004880*200
tau.hat.st <- sum(sum(samp1), sum(samp2), sum(samp3))
mu.hat.st <- tau.hat.st/sum(N)
mu.hat.st
sum(samp1)
sum(samp2)
sum(samp3)
tau.hat.1 <- (N[1]*samp1)/n[1]
tau.hat.2 <- (N[2]*samp1)/n[2]
tau.hat.3 <- (N[3]*samp1)/n[3]
tau.hat.st <- sum(tau.hat.1, tau.hat.2, tau.hat.3)
mu.hat.st <- tau.hat.st/sum(N)
mu.hat.st
(1000+1000+9000)/450
v <- (1/450^2)*((280000)+(1500*600))
v
sqrt(v)*1.96
(1/sum(N))
sum(N)
sum(samp1)
(N[1]*sum(samp1))/n[1]
1140/14
(1140/14)*50
tau.hat.1 <- (N[1]*sum(samp1))/n[1]
tau.hat.2 <- (N[2]*sum(samp2))/n[2]
tau.hat.3 <- (N[3]*sum(samp3))/n[3]
tau.hat.st <- sum(tau.hat.1, tau.hat.2, tau.hat.3)
mu.hat.st <- tau.hat.st/sum(N)
mu.hat.st
sd(samp1)
var(samp1)
var.hat.mu.hat <- (1/(sum(N))^2)*((N[1]*(N[1]-n[1])*var(samp1)/n[1])+
(N[2]*(N[2]-n[2])*var(samp1)/n[2])+
(N[3]*(N[3]-n[3])*var(samp1)/n[3]))
var.hat.mu.hat
(sum(N)
)
1/(sum(N))^2
var.hat.mu.hat <- (1/(sum(N))^2)*(((N[1]*(N[1]-n[1])*var(samp1))/n[1])+
((N[2]*(N[2]-n[2])*var(samp1))/n[2])+
((N[3]*(N[3]-n[3])*var(samp1))/n[3]))
var.hat.mu.hat
a1 <- ((N[1]*(N[1]-n[1]))/n[1])
a2 <- ((N[2]*(N[2]-n[2]))/n[2])
a3 <- ((N[3]*(N[3]-n[3]))/n[3])
a1 <- ((N[1]*(N[1]-n[1]))/n[1])
a2 <- ((N[2]*(N[2]-n[2]))/n[2])
a3 <- ((N[3]*(N[3]-n[3]))/n[3])
v1 <- var(samp1)
v2 <- var(samp2)
v3 <- var(samp3)
d <- ((a1*v1)+(a2*v2)+(a3*v3))^2/
(((a1*v1)^2/(n[1]-1))+((a2*v2)^2/(n[2]-1))+((a3*v3)^2/(n[3]-1)))
d
alpha <- 0.95
tval <- abs(qt(p = (1-alpha)/2, df = d))
tval
tval*sqrt(var.hat.mu.hat)
sd(samp1)
sd(samp2)
sd(samp3)
(68*sd(samp1))+(80*sd(samp2))+(42*sd(samp3))
(68*sd(samp1))+(80*sd(samp2))+(42*sd(samp3))
meh <- (68*sd(samp1))+(80*sd(samp2))+(42*sd(samp3))
(50*68*sd(samp1))/meh
(50*80*sd(samp2))/meh
(50*42*sd(samp3))/meh
((520*650)+(420*350))/1000
(1/1000^2)*(((650*(650-22)*20^2)/22) + ((350*(350-42)*50^2)/42))
dt <- c(12,15,8,13,18,7,6,13,22,9.8,7,18,12.34,5,8.9,14,4,11.4,5,13,8.9,8.7,10,9.2)
mean(dt)
sd(dt)
sqrt(13175)
rm(list=ls())
gc()
set.seed(312)
library(doParallel)
library(fields)
library(foreach)
library(magic)
library(permute)
rm(list=ls())
gc()
set.seed(312)
library(doParallel)
library(fields)
library(foreach)
library(magic)
library(permute)
## Data
setwd("~/Dropbox/Interference_in_Field_Experiments/Analysis/Broockman_dataverse_repository/For_cluster")
broockman.data <- read.table("broockman_intrinsic_motivation_1.tab", sep="\t", header=TRUE)
broockman.data <- broockman.data[is.na(broockman.data$demvotepercent) == FALSE,]
permute.within.categories <- function(categories,z){
ucategories <- unique(categories)
perm.z <- rep(NA,length(z))
for(c in ucategories){
z.c <- z[which(categories==c)]
perm.z.c <- sample(z.c,length(z.c),rep=F)
perm.z[which(categories==c)] <- perm.z.c
}
perm.z
}
get.similarity <- function(x, y){
return((2-abs(x-y))/2)
}
z <- broockman.data$treat_out #observed treatment
y.z <- broockman.data$code_some_response_given #observed outcome
n <- length(y.z) #number of observations
t <- length(z[z==1]) #number of treated units
adj.mat <- NA
for (i in 1:length(unique(broockman.data$leg_state))){
state.mat <- matrix(NA, nrow =length(broockman.data[broockman.data$leg_state==unique(broockman.data$leg_state)[i],1]), ncol = length(broockman.data[broockman.data$leg_state==unique(broockman.data$leg_state)[i],1]))
for (j in 1:length(broockman.data[broockman.data$leg_state==unique(broockman.data$leg_state)[i],1])){
for (k in 1:length(broockman.data[broockman.data$leg_state==unique(broockman.data$leg_state)[i],1])){
state.mat[j,k] <- get.similarity(broockman.data$demvotepercent[j], broockman.data$demvotepercent[k])
diag(state.mat) <- 0
}
}
adj.mat <- adiag(adj.mat, state.mat)
}
adj.mat <- adj.mat[-1,-1]
perms <- 1000 #number of permutations to use in generating expected exposure
perm <- replicate(perms, permute.within.categories(broockman.data$leg_state,z))
expected.exp0 <- rep(0, n)
expected.exp1 <- rep(0, n)
for(p in 1:ncol(perm)){
#zp <- permute.within.categories(data$match_category,z)
zp <- perm[,p]
for(i in 1:n){
if (zp[i] == 1){
expected.exp1[i] <- expected.exp1[i] + sum(adj.mat[i,]*zp)
}else{
expected.exp0[i] <- expected.exp0[i] + sum(adj.mat[i,]*zp)
}
}
}
num_treat <- apply(perm,1,sum)
num_control <- apply(1-perm,1,sum)
expected.exp1 <- expected.exp1/num_treat
expected.exp0 <- expected.exp0/num_control
indirect.treatment <- function(permutation, adj.mat){ #any treatment assignment vector and adjacency matrix can be used
# permutation: can be the initial treatment assignment or a permutation
raw.exp <- rep(NA, n)
for (i in 1:n){
raw.exp[i] <- sum(adj.mat[i,]*permutation) #apply function
}
net.exp <- raw.exp - (permutation*expected.exp1 + (1-permutation)*expected.exp0)
standard.exp <- (net.exp - mean(net.exp))/sd(net.exp) #this is the spillover or indirect effect
return(standard.exp)
}
z.to.unif <- function(outcome, beta1, beta2, permutation, adj.mat){
# outcome: vector of direct treatment outcomes
# beta1: direct treatment effect parameter
# beta2: indirect treatment effect parameter
# permutation: vector of a permutation of z (can be z itself)
# adj.mat: adjacency matrix
exposure <- indirect.treatment(permutation, adj.mat)
# This is equation 5
h.yz.0 <- outcome - (beta1*permutation) - (beta2*exposure)
return(h.yz.0)
}
perms.test <- 1 #number of permutations used in testing
beta1s <- seq(from=-0.1, to=0.1, by=.25)
beta2s <- seq(from=-0.1, to=0.1, by=.25)
pvals <- matrix(NA, length(beta1s), length(beta2s))
#Use a package called Parallel
cl <- parallel::makeCluster(20) #Setup for parallel computing
registerDoParallel(cl)
start.time <- Sys.time()
pvalues.ideology <- foreach (i = 1:length(beta1s), .packages = "foreach") %dopar% {
abc <- foreach (j = 1:length(beta2s)) %dopar% {
# Calculate observed test statistic
exposure <- indirect.treatment(permutation = z, adj.mat = adj.mat)
test.stat <- sum((lm(y.z ~ z + exposure, na.action = na.omit)$resid)^2)
# Calculate a vector of test statistic using permutations
results <- foreach (k = 1:perms.test) %dopar% {
require(permute)
perm.z <- permute.within.categories(broockman.data$leg_state,z)
perm.exposure <- indirect.treatment(permutation = perm.z, adj.mat = adj.mat)
perm.y.0 <- y.z + (-1 * beta2s[j] * indirect.treatment(permutation = z, adj.mat = adj.mat))
perm.y.0[z==1] <- perm.y.0[z==1] - beta1s[i]
y.sim <- perm.y.0 + beta1s[i]*perm.z + beta2s[j]*perm.exposure
perm.test.stat <- sum((lm(y.sim ~ perm.z + perm.exposure, na.action = na.omit)$resid)^2)
}
# A vector of test statistics
all.test.stat.vals <- as.numeric(unlist(results))
# Calculating p-value
pval <- sum(all.test.stat.vals < test.stat)/perms.test
}
as.numeric(unlist(abc))
}
end.time <- Sys.time()
time <- end.time - start.time
time
stopCluster(cl)
perms.test <- 10 #number of permutations used in testing
beta1s <- seq(from=-0.1, to=0.1, by=.25)
beta2s <- seq(from=-0.1, to=0.1, by=.25)
pvals <- matrix(NA, length(beta1s), length(beta2s))
#Use a package called Parallel
cl <- parallel::makeCluster(20) #Setup for parallel computing
registerDoParallel(cl)
start.time <- Sys.time()
pvalues.ideology <- foreach (i = 1:length(beta1s), .packages = "foreach") %dopar% {
abc <- foreach (j = 1:length(beta2s)) %dopar% {
# Calculate observed test statistic
exposure <- indirect.treatment(permutation = z, adj.mat = adj.mat)
test.stat <- sum((lm(y.z ~ z + exposure, na.action = na.omit)$resid)^2)
# Calculate a vector of test statistic using permutations
results <- foreach (k = 1:perms.test) %dopar% {
require(permute)
perm.z <- permute.within.categories(broockman.data$leg_state,z)
perm.exposure <- indirect.treatment(permutation = perm.z, adj.mat = adj.mat)
perm.y.0 <- y.z + (-1 * beta2s[j] * indirect.treatment(permutation = z, adj.mat = adj.mat))
perm.y.0[z==1] <- perm.y.0[z==1] - beta1s[i]
y.sim <- perm.y.0 + beta1s[i]*perm.z + beta2s[j]*perm.exposure
perm.test.stat <- sum((lm(y.sim ~ perm.z + perm.exposure, na.action = na.omit)$resid)^2)
}
# A vector of test statistics
all.test.stat.vals <- as.numeric(unlist(results))
# Calculating p-value
pval <- sum(all.test.stat.vals < test.stat)/perms.test
}
as.numeric(unlist(abc))
}
end.time <- Sys.time()
time <- end.time - start.time
time
stopCluster(cl)
perms.test <- 100 #number of permutations used in testing
beta1s <- seq(from=-0.1, to=0.1, by=.25)
beta2s <- seq(from=-0.1, to=0.1, by=.25)
pvals <- matrix(NA, length(beta1s), length(beta2s))
#Use a package called Parallel
cl <- parallel::makeCluster(20) #Setup for parallel computing
registerDoParallel(cl)
start.time <- Sys.time()
pvalues.ideology <- foreach (i = 1:length(beta1s), .packages = "foreach") %dopar% {
abc <- foreach (j = 1:length(beta2s)) %dopar% {
# Calculate observed test statistic
exposure <- indirect.treatment(permutation = z, adj.mat = adj.mat)
test.stat <- sum((lm(y.z ~ z + exposure, na.action = na.omit)$resid)^2)
# Calculate a vector of test statistic using permutations
results <- foreach (k = 1:perms.test) %dopar% {
require(permute)
perm.z <- permute.within.categories(broockman.data$leg_state,z)
perm.exposure <- indirect.treatment(permutation = perm.z, adj.mat = adj.mat)
perm.y.0 <- y.z + (-1 * beta2s[j] * indirect.treatment(permutation = z, adj.mat = adj.mat))
perm.y.0[z==1] <- perm.y.0[z==1] - beta1s[i]
y.sim <- perm.y.0 + beta1s[i]*perm.z + beta2s[j]*perm.exposure
perm.test.stat <- sum((lm(y.sim ~ perm.z + perm.exposure, na.action = na.omit)$resid)^2)
}
# A vector of test statistics
all.test.stat.vals <- as.numeric(unlist(results))
# Calculating p-value
pval <- sum(all.test.stat.vals < test.stat)/perms.test
}
as.numeric(unlist(abc))
}
end.time <- Sys.time()
time <- end.time - start.time
time
stopCluster(cl)
perms.test <- 100 #number of permutations used in testing
beta1s <- seq(from=-0.2, to=0.2, by=.25)
beta2s <- seq(from=-0.2, to=0.2, by=.25)
pvals <- matrix(NA, length(beta1s), length(beta2s))
#Use a package called Parallel
cl <- parallel::makeCluster(20) #Setup for parallel computing
registerDoParallel(cl)
start.time <- Sys.time()
pvalues.ideology <- foreach (i = 1:length(beta1s), .packages = "foreach") %dopar% {
abc <- foreach (j = 1:length(beta2s)) %dopar% {
# Calculate observed test statistic
exposure <- indirect.treatment(permutation = z, adj.mat = adj.mat)
test.stat <- sum((lm(y.z ~ z + exposure, na.action = na.omit)$resid)^2)
# Calculate a vector of test statistic using permutations
results <- foreach (k = 1:perms.test) %dopar% {
require(permute)
perm.z <- permute.within.categories(broockman.data$leg_state,z)
perm.exposure <- indirect.treatment(permutation = perm.z, adj.mat = adj.mat)
perm.y.0 <- y.z + (-1 * beta2s[j] * indirect.treatment(permutation = z, adj.mat = adj.mat))
perm.y.0[z==1] <- perm.y.0[z==1] - beta1s[i]
y.sim <- perm.y.0 + beta1s[i]*perm.z + beta2s[j]*perm.exposure
perm.test.stat <- sum((lm(y.sim ~ perm.z + perm.exposure, na.action = na.omit)$resid)^2)
}
# A vector of test statistics
all.test.stat.vals <- as.numeric(unlist(results))
# Calculating p-value
pval <- sum(all.test.stat.vals < test.stat)/perms.test
}
as.numeric(unlist(abc))
}
end.time <- Sys.time()
time <- end.time - start.time
time
stopCluster(cl)
perms.test <- 100 #number of permutations used in testing
beta1s <- seq(from=-0.3, to=0.3, by=.125)
beta2s <- seq(from=-0.3, to=0.3, by=.125)
pvals <- matrix(NA, length(beta1s), length(beta2s))
#Use a package called Parallel
cl <- parallel::makeCluster(20) #Setup for parallel computing
registerDoParallel(cl)
start.time <- Sys.time()
pvalues.ideology <- foreach (i = 1:length(beta1s), .packages = "foreach") %dopar% {
abc <- foreach (j = 1:length(beta2s)) %dopar% {
# Calculate observed test statistic
exposure <- indirect.treatment(permutation = z, adj.mat = adj.mat)
test.stat <- sum((lm(y.z ~ z + exposure, na.action = na.omit)$resid)^2)
# Calculate a vector of test statistic using permutations
results <- foreach (k = 1:perms.test) %dopar% {
require(permute)
perm.z <- permute.within.categories(broockman.data$leg_state,z)
perm.exposure <- indirect.treatment(permutation = perm.z, adj.mat = adj.mat)
perm.y.0 <- y.z + (-1 * beta2s[j] * indirect.treatment(permutation = z, adj.mat = adj.mat))
perm.y.0[z==1] <- perm.y.0[z==1] - beta1s[i]
y.sim <- perm.y.0 + beta1s[i]*perm.z + beta2s[j]*perm.exposure
perm.test.stat <- sum((lm(y.sim ~ perm.z + perm.exposure, na.action = na.omit)$resid)^2)
}
# A vector of test statistics
all.test.stat.vals <- as.numeric(unlist(results))
# Calculating p-value
pval <- sum(all.test.stat.vals < test.stat)/perms.test
}
as.numeric(unlist(abc))
}
end.time <- Sys.time()
time <- end.time - start.time
time
stopCluster(cl)
perms.test <- 500 #number of permutations used in testing
beta1s <- seq(from=-0.4, to=0.4, by=.125)
beta2s <- seq(from=-0.4, to=0.4, by=.125)
pvals <- matrix(NA, length(beta1s), length(beta2s))
#Use a package called Parallel
cl <- parallel::makeCluster(20) #Setup for parallel computing
registerDoParallel(cl)
start.time <- Sys.time()
pvalues.ideology <- foreach (i = 1:length(beta1s), .packages = "foreach") %dopar% {
abc <- foreach (j = 1:length(beta2s)) %dopar% {
# Calculate observed test statistic
exposure <- indirect.treatment(permutation = z, adj.mat = adj.mat)
test.stat <- sum((lm(y.z ~ z + exposure, na.action = na.omit)$resid)^2)
# Calculate a vector of test statistic using permutations
results <- foreach (k = 1:perms.test) %dopar% {
require(permute)
perm.z <- permute.within.categories(broockman.data$leg_state,z)
perm.exposure <- indirect.treatment(permutation = perm.z, adj.mat = adj.mat)
perm.y.0 <- y.z + (-1 * beta2s[j] * indirect.treatment(permutation = z, adj.mat = adj.mat))
perm.y.0[z==1] <- perm.y.0[z==1] - beta1s[i]
y.sim <- perm.y.0 + beta1s[i]*perm.z + beta2s[j]*perm.exposure
perm.test.stat <- sum((lm(y.sim ~ perm.z + perm.exposure, na.action = na.omit)$resid)^2)
}
# A vector of test statistics
all.test.stat.vals <- as.numeric(unlist(results))
# Calculating p-value
pval <- sum(all.test.stat.vals < test.stat)/perms.test
}
as.numeric(unlist(abc))
}
rm(list=ls())
gc()
set.seed(312)
sample <- c(12.06,
11.97,
11.89,
11.76,
11.87,
11.90,
11.91,
11.98,
11.79,
11.98,
12.00,
11.93,
11.87,
12.02,
11.78,
12.09,
12.39,
12.03,
11.93,
12.04,
12.07,
12.05,
12.08,
12.10,
12.18,
12.01,
12.19,
12.09,
12.21,
12.31,
12.22,
12.16,
12.23,
12.13,
12.24,
12.29,
12.31,
12.35,
12.32,
12.33,
12.37,
12.36
)
